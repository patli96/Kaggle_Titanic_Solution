{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn import feature_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\npd.set_option('display.max_columns', 200)\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-23T14:56:42.942074Z","iopub.execute_input":"2022-04-23T14:56:42.943851Z","iopub.status.idle":"2022-04-23T14:56:44.467944Z","shell.execute_reply.started":"2022-04-23T14:56:42.943757Z","shell.execute_reply":"2022-04-23T14:56:44.467208Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Inspection\n\n## 1. Overview\n- `PassengerId`: the unique id of each passenger\n- `Survived`: the target we need to predict:\n    - 1 = Survived\n    - 0 = Not Survived\n- `Pclass`: the class level of each passenger:\n    - 1 = Upper Class\n    - 2 = Middle Class\n    - 3 = Lower Class\n- `Name`: passenger's name, in a format of `<Surname>, <Title>. <MiddleName LastName>`\n- `Sex`: passenger's gender (male or female)\n- `Age`: passenger's age\n- `SibSp`:the number of each passenger's siblings and spouse\n- `Parch`: the number of each passenger's parents and children\n- `Ticket`: the ticket code of the passenger\n- `Fare`: passenger's fare\n- `Cabin`: the cabin number of the passenger, in a format of `<A-G,T>(<0-9>)+`\n- `Embarked`: the port of embarkation:\n    - C = Cherbourg\n    - Q = Queenstown\n    - S = Southampton\n\n\n## 2. Missing Values\n","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/titanic/train.csv')\ntest_df = pd.read_csv('../input/titanic/test.csv')\n\nprint(\"Training set missing values:\")\nfor col in train_df.drop(columns=['Survived']).columns:\n    print(f\"{col}: {train_df[train_df[col].isnull()].shape[0]}\")\nprint()\nprint(\"Test set missing values:\")\nfor col in test_df.columns:\n    print(f\"{col}: {test_df[test_df[col].isnull()].shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:44.469458Z","iopub.execute_input":"2022-04-23T14:56:44.469665Z","iopub.status.idle":"2022-04-23T14:56:44.521451Z","shell.execute_reply.started":"2022-04-23T14:56:44.469640Z","shell.execute_reply":"2022-04-23T14:56:44.519715Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 3. Exploration\n\n### 3.1 Pclass\n\nFrom the chart shown below, we can tell people from the first class have the highest survival rate while passengers from the thrid class have the lowest survival rate.","metadata":{}},{"cell_type":"code","source":"sns.barplot(data=train_df,x='Pclass',y='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:44.522994Z","iopub.execute_input":"2022-04-23T14:56:44.523287Z","iopub.status.idle":"2022-04-23T14:56:44.758945Z","shell.execute_reply.started":"2022-04-23T14:56:44.523247Z","shell.execute_reply":"2022-04-23T14:56:44.758280Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Name\n\nWe can extract titles from passengers' names, such as 'Mr.', 'Mrs', 'Miss.', 'Dr.', etc.\n\nHere are all the unique titles I extracted from passengers' names:\n\n{'Miss', 'the Countess', 'Jonkheer', 'Lady', 'Mme', 'Major', 'Dona', 'Ms', 'Mrs', 'Mr', 'Mlle', 'Capt', 'Rev', 'Dr', 'Master', 'Col', 'Sir', 'Don'}","metadata":{}},{"cell_type":"code","source":"df = pd.concat([train_df.drop(columns=['Survived']), test_df])\ndf['title_tmp'] = df['Name'].str.split(',').str[1]\ndf['Title'] = df['title_tmp'].str.strip().str.split('.').str[0]\ndf.drop(columns=['title_tmp'], inplace=True)\nprint(set(df['Title'].values))","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:44.760498Z","iopub.execute_input":"2022-04-23T14:56:44.760958Z","iopub.status.idle":"2022-04-23T14:56:44.781309Z","shell.execute_reply.started":"2022-04-23T14:56:44.760917Z","shell.execute_reply":"2022-04-23T14:56:44.780469Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"We can have a look at the survival rate of each title:","metadata":{"execution":{"iopub.status.busy":"2022-04-23T11:44:12.214904Z","iopub.execute_input":"2022-04-23T11:44:12.216100Z","iopub.status.idle":"2022-04-23T11:44:12.221080Z","shell.execute_reply.started":"2022-04-23T11:44:12.216060Z","shell.execute_reply":"2022-04-23T11:44:12.220229Z"}}},{"cell_type":"code","source":"train_df['Title'] = df.iloc[:train_df.shape[0]]['Title']\nsns.set(rc={'figure.figsize':(23,8)})\nsns.barplot(data=train_df,x='Title',y='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:45.476068Z","iopub.execute_input":"2022-04-23T14:56:45.476352Z","iopub.status.idle":"2022-04-23T14:56:45.975168Z","shell.execute_reply.started":"2022-04-23T14:56:45.476320Z","shell.execute_reply":"2022-04-23T14:56:45.974370Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We can bin all the titles into 4 buckets: 'Mr', 'Mrs', 'Ms', 'Miss' and 'Special', \n\nwhere 'Special' contains: 'the Countess', 'Jonkheer', 'Lady', 'Mme', 'Major', 'Dona', 'Mlle', 'Capt', 'Rev', 'Dr', 'Master', 'Col', 'Sir', 'Don'","metadata":{}},{"cell_type":"code","source":"df['TitleBinned'] = df['Name'].apply(lambda x: 'Mr' if 'Mr.' in x else ('Mrs' if 'Mrs.' in x else ('Miss' if 'Miss.' in x else ('Ms' if 'Ms' in x else 'Special'))))\ntrain_df['TitleBinned'] = df.iloc[:train_df.shape[0]]['TitleBinned']\nsns.set(rc={'figure.figsize':(8,6)})\nsns.barplot(data=train_df,x='TitleBinned',y='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:45.976728Z","iopub.execute_input":"2022-04-23T14:56:45.977434Z","iopub.status.idle":"2022-04-23T14:56:46.230767Z","shell.execute_reply.started":"2022-04-23T14:56:45.977388Z","shell.execute_reply":"2022-04-23T14:56:46.230140Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 SibSp and Parch\n\nHere's the survival rate of `SibSp` and `Parch`:","metadata":{}},{"cell_type":"code","source":"sns.barplot(data=train_df,x='SibSp',y='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:46.580428Z","iopub.execute_input":"2022-04-23T14:56:46.580697Z","iopub.status.idle":"2022-04-23T14:56:46.909481Z","shell.execute_reply.started":"2022-04-23T14:56:46.580668Z","shell.execute_reply":"2022-04-23T14:56:46.908615Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"sns.barplot(data=train_df,x='Parch',y='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:47.108340Z","iopub.execute_input":"2022-04-23T14:56:47.108877Z","iopub.status.idle":"2022-04-23T14:56:47.418017Z","shell.execute_reply.started":"2022-04-23T14:56:47.108839Z","shell.execute_reply":"2022-04-23T14:56:47.417250Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"I tried to aggregate these two columns into one called `FamilySize`, which indicate the number of family members on board of each passenger.\n\nWe can see passenger with a family size of 2-4 tend to has a higher survival rate.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\nsns.barplot(data=train_df,x='FamilySize',y='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:48.605238Z","iopub.execute_input":"2022-04-23T14:56:48.606137Z","iopub.status.idle":"2022-04-23T14:56:48.991179Z","shell.execute_reply.started":"2022-04-23T14:56:48.606078Z","shell.execute_reply":"2022-04-23T14:56:48.990275Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 3.4 Ticket\n\nThere are cases that some passnegers travelled with friends or other people not from the passenger's family, which won't be told from `FamilySize`.\n\n\n`Ticket` provides a way to figure this out.\n\nWe can identify passengers with the same ticket code:","metadata":{}},{"cell_type":"code","source":"ticket_dict = train_df['Ticket'].value_counts()\ntrain_df['GroupSize'] = train_df['Ticket'].map(ticket_dict)\nsns.barplot(data=train_df,x='GroupSize',y='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:50.238526Z","iopub.execute_input":"2022-04-23T14:56:50.238819Z","iopub.status.idle":"2022-04-23T14:56:50.821603Z","shell.execute_reply.started":"2022-04-23T14:56:50.238783Z","shell.execute_reply":"2022-04-23T14:56:50.820329Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:09:29.878703Z","iopub.execute_input":"2022-04-23T12:09:29.878997Z","iopub.status.idle":"2022-04-23T12:09:29.929615Z","shell.execute_reply.started":"2022-04-23T12:09:29.878966Z","shell.execute_reply":"2022-04-23T12:09:29.928562Z"}}},{"cell_type":"markdown","source":"From the chart above, we can tell that passenger with a group size of 2-4 has higher survival rate.","metadata":{}},{"cell_type":"markdown","source":"### 3.5 Fare\n\n\nThe fare of each passenger has a relation to the survival possibility.\n\nFrom Part 2 we can tell there are some missing values in the `Fare` column.\n\nSince it has something to do with passenger's `Pclass`, I filled `NaN` in `Fare` column with the median number of passengers from the same `Pclass`.\n\nThen, I binned fare values into 5 buckets:","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\n# Fill NaN\npclass_set = {1, 2, 3}\nmedian_fare_dict = train_df.groupby('Pclass')['Fare'].median().to_dict()\nfor pclass in pclass_set:\n    train_df.loc[(train_df['Fare'].isnull()) & (train_df['Pclass'] == pclass), 'Fare'] = median_fare_dict[pclass]","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:52.843412Z","iopub.execute_input":"2022-04-23T14:56:52.844275Z","iopub.status.idle":"2022-04-23T14:56:52.855589Z","shell.execute_reply.started":"2022-04-23T14:56:52.844227Z","shell.execute_reply":"2022-04-23T14:56:52.854882Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def bin_fare(train_df, n_bins):\n    # Binning\n    train_df['fare_tmp'] = pd.qcut(train_df['Fare'], n_bins)\n    label = preprocessing.LabelEncoder()\n    train_df['FareBinned'] = label.fit_transform(train_df['fare_tmp'])\n    train_df.drop(columns=['fare_tmp'], inplace=True)\n    # Plotting\n    sns.barplot(data=train_df,x='FareBinned',y='Survived')\n    return True","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:53.411297Z","iopub.execute_input":"2022-04-23T14:56:53.412062Z","iopub.status.idle":"2022-04-23T14:56:53.417412Z","shell.execute_reply.started":"2022-04-23T14:56:53.412015Z","shell.execute_reply":"2022-04-23T14:56:53.416622Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"bin_fare(train_df, 5)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:53.649269Z","iopub.execute_input":"2022-04-23T14:56:53.649770Z","iopub.status.idle":"2022-04-23T14:56:53.936610Z","shell.execute_reply.started":"2022-04-23T14:56:53.649733Z","shell.execute_reply":"2022-04-23T14:56:53.936083Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"We can try to bin it into various numbers of buckets.","metadata":{}},{"cell_type":"code","source":"bin_fare(train_df, 6)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:54.487220Z","iopub.execute_input":"2022-04-23T14:56:54.487875Z","iopub.status.idle":"2022-04-23T14:56:54.854952Z","shell.execute_reply.started":"2022-04-23T14:56:54.487839Z","shell.execute_reply":"2022-04-23T14:56:54.854103Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"bin_fare(train_df, 4)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:54.856894Z","iopub.execute_input":"2022-04-23T14:56:54.857227Z","iopub.status.idle":"2022-04-23T14:56:55.155680Z","shell.execute_reply.started":"2022-04-23T14:56:54.857167Z","shell.execute_reply":"2022-04-23T14:56:55.154900Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### 3.6 Cabin\n\n\nThe first letter in`Cabin` number indicates the location of passenger's deck, which could be a useful factor to determine whether a passenger would survive in the end.\n\nHere are all the `Cabin` letters in the data set: `{'A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', nan}`\n\nFrom Part 2 we know there are lots of missing values in `Cabin`.\n\nI will mark missing values as `U` for now, and let's see how it looks in terms of the survival rate of passengers from each deck:\n","metadata":{}},{"cell_type":"code","source":"# Some passengers might have multiple cabin numbers, I will use the last one in this case\ntrain_df['Cabin'] = train_df['Cabin'].fillna('U')\ntrain_df['LastCabin'] = train_df['Cabin'].str.split(' ').str[-1]\ntrain_df['LastCabinLetter'] = train_df['LastCabin'].str[0]\ntrain_df.loc[train_df['LastCabinLetter'] == 'T', 'LastCabinLetter'] = 'A'\nsns.barplot(data=train_df,x='LastCabinLetter',y='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:56.470597Z","iopub.execute_input":"2022-04-23T14:56:56.470853Z","iopub.status.idle":"2022-04-23T14:56:56.884265Z","shell.execute_reply.started":"2022-04-23T14:56:56.470827Z","shell.execute_reply":"2022-04-23T14:56:56.883536Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"We can take a further look at the relation between `Cabin` and `Pclass`","metadata":{}},{"cell_type":"code","source":"train_df['Cabin'] = train_df['Cabin'].fillna('U')\ntrain_df['LastCabin'] = train_df['Cabin'].str.split(' ').str[-1]\ntrain_df['LastCabinLetter'] = train_df['LastCabin'].str[0]\nfor cabin in sorted(list(set(train_df['LastCabinLetter'].values))):\n    print(f\"Cabin {cabin}:\")\n    print(train_df[train_df['LastCabinLetter'] == cabin]['Pclass'].value_counts())\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:56:59.589683Z","iopub.execute_input":"2022-04-23T14:56:59.589945Z","iopub.status.idle":"2022-04-23T14:56:59.616133Z","shell.execute_reply.started":"2022-04-23T14:56:59.589921Z","shell.execute_reply":"2022-04-23T14:56:59.615411Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"In conclusion:\n- Deck `A`, `B`, `C`: All from class `1`\n- Deck `D`: Most from class `1` and some class `2`\n- Deck `E`: Most from class `1` and some from class `2`, `3`\n- Deck `F`: Most from class `2` and some from class `3`\n- Deck `G`: All from class `3`\n- Deck `T`: All from class `1`\n- Deck `U`: Most from class `3`, some from class `2`, `3`\n\nSince there's only one passenger on deck `T`, I moved it to deck `A` instead.","metadata":{}},{"cell_type":"markdown","source":"### 3.7 Embarked\n\nHere's the survival rate of passengers embarked at different port\n","metadata":{}},{"cell_type":"code","source":"sns.barplot(data=train_df[train_df['Embarked'].notnull()],x='Embarked',y='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:57:05.401643Z","iopub.execute_input":"2022-04-23T14:57:05.401912Z","iopub.status.idle":"2022-04-23T14:57:05.665441Z","shell.execute_reply.started":"2022-04-23T14:57:05.401884Z","shell.execute_reply":"2022-04-23T14:57:05.664616Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\n\nBased on the findings above, here's how I'm going to construct features for the training set:\n- Fill `NaN` in `Cabin` columns with 'H' and covert the first letter of `Cabin` column to an integer\n- Get title of each passenger, which are `Mr`, `Mrs`, `Ms`, `Miss` and `Special`, then map it to an integer\n- Calculate the `FamilySize` for each passenger by: `SibSp` + `Parch` + 1\n- Calculate the size of group travelling together (`GroupSize`) for each passenger since some people may travel with friends instead of with family\n- Fill `NaN` in `Fare` column with the median number of passengers from the same `Pclass` and bin `Fare` column into `n` buckets\n- Fill `NaN` in `Embarked` column with the most frequent value from passengers with the same `FareBinned` and `Pclass`\n- Fill `NaN` in `Age` column with the median number of passengers with the same title and bin `Age` column into `n` buckets\n","metadata":{}},{"cell_type":"markdown","source":"### A final touch on the top\n\nI've seen lots of notebooks mentioned that we could calculate a `FamilySurvival` column to the dataset.\n\nTo calculate this value, we need to get last name of each passenger, and then group the dataframe by`LastName, Fare`.\n\nFirst, we set the default value of `FamilySurvival` to `0.5`.\n\nIf the number of rows in the group is greater than 1, and if there's any family member in the group survived, then set `FamilySurvival` to `1`.\n\nIf the number of rows in the group is greater than 1, and if there's no family member in the group survived, then set `FamilySurvival` to `0`.\n\n","metadata":{}},{"cell_type":"markdown","source":"### Pearson Correlation Heatmap\n\nFrom the Pearson Correlation plot below we can tell that there are not too many features strongly correlated with one another, which means that there are not much redundant features in my training set.\n\nThe highest absolute value of correlation in from the plot:\n1. `GroupSize` and `FamilySize`: 0.82\n2. `Pclass` and `LastCabinLetter`: 0.75\n3. `Pclass` and `FareBinned`: 0.69","metadata":{}},{"cell_type":"code","source":"def add_cabin_letter(df):\n    \"\"\"\n    Covert the first letter of 'Cabin' column to an integer, and fill NaN with 0\n\n    :param df: pd.df\n    :return: pd.df with 'CabinLetter' column\n    \"\"\"\n    df['Cabin'] = df['Cabin'].fillna('H')\n    df['LastCabin'] = df['Cabin'].str.split(' ').str[-1]\n    df['LastCabinLetter'] = df['LastCabin'].str[0]\n    df.loc[df['LastCabinLetter'] == 'T', 'LastCabinLetter'] = 'A'\n    df['LastCabinLetter'] = df['LastCabinLetter'].apply(lambda x: ord(x) - ord('A') + 1)\n    df.drop(columns=['LastCabin'], inplace=True)\n    return df\n\n\ndef add_title(df):\n    \"\"\"\n    Get title of each passenger and map it to an integer\n\n    :param df: pd.df\n    :return: pd.df with 'Title' columns\n    \"\"\"\n    df['TitleTmp'] = df['Name'].apply(lambda x: 'Mr' if 'Mr.' in x else ('Mrs' if 'Mrs.' in x else ('Miss' if 'Miss.' in x else 'Special')))\n    label = preprocessing.LabelEncoder()\n    df['Title'] = label.fit_transform(df['TitleTmp'])\n    df.drop(columns=['TitleTmp'], inplace=True)\n    return df\n\n\ndef add_family_size(df):\n    \"\"\"\n    Calculate the size of family for each passenger\n\n    :param df: pd.df\n    :return: pd.df with 'FamilySize' column\n    \"\"\"\n    df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n    return df\n\n\ndef add_group_size(df):\n    \"\"\"\n    Calculate the size of group travelling together for each passenger\n    since some people may travel with friends instead of with family\n\n    :param df: pd.df\n    :return: pd.df with 'GroupSize' column\n    \"\"\"\n    ticket_dict = df['Ticket'].value_counts()\n    df['GroupSize'] = df['Ticket'].map(ticket_dict)\n    return df\n\n\ndef process_fare(df, n_bins):\n    \"\"\"\n    1. Fill NaN in 'Fare' column with the median number of passengers from the same 'Pclass'\n    2. Bin 'Fare' column into 5 buckets\n\n    :param df: pd.df\n    :param n_bins: number of bins\n    :return: pd.df with 'FareBinned' column\n    \"\"\"\n    # Fill NaN\n    pclass_set = {1, 2, 3}\n    median_fare_dict = df.groupby('Pclass')['Fare'].median().to_dict()\n    for pclass in pclass_set:\n        df.loc[(df['Fare'].isnull()) & (df['Pclass'] == pclass), 'Fare'] = median_fare_dict[pclass]\n    # Binning\n    df['fare_tmp'] = pd.qcut(df['Fare'], n_bins)\n    label = preprocessing.LabelEncoder()\n    df['FareBinned'] = label.fit_transform(df['fare_tmp'])\n    df.drop(columns=['fare_tmp'], inplace=True)\n    return df\n\n\ndef fill_nan_in_embarked(df):\n    \"\"\"\n    Fill NaN in 'Embarked' column with the most frequent value from passengers with the same 'FareBinned' and 'Pclass'\n\n    :param df: pd.df\n    :return: pd.df\n    \"\"\"\n    nan_embarked_df = df[df['Embarked'].isnull()]\n    for idx, row in nan_embarked_df.iterrows():\n        df.loc[idx, 'Embarked'] = df[(df['Pclass'] == row['Pclass']) & (df['FareBinned'] == row['FareBinned'])]['Embarked']\\\n            .value_counts().idxmax()\n    return df\n\n\ndef process_age(df, n_bins):\n    \"\"\"\n    Fill NaN in 'Age' column with the median number of passengers with the same title\n    By inspection, it is guaranteed that every title with missing 'Age' has at least one record with a valid 'Age'\n    Then Bin 'Age' into 5 buckets\n\n    :param df: pd.df\n    :param n_bins: number of bins\n    :return: pd.df\n    \"\"\"\n    # Add 'title_tmp' column to get the actual title of each passenger\n    df['tmp'] = df['Name'].str.split(',').str[1]\n    df['title_tmp'] = df['tmp'].str.strip().str.split('.').str[0]\n    # Get a df of passengers with NaN in 'Age'\n    nan_age_df = df[df['Age'].isnull()]\n    nan_age_titles = set(nan_age_df['title_tmp'].values)\n    # Get the median number of Age from passengers with the same 'title_tmp' value\n    median_age_dict = {title: df[(df['title_tmp'] == title) & (df['Age'].notnull())]['Age'].median()\n                       for title in nan_age_titles}\n    # Fill NaN in 'Age'\n    for idx, row in nan_age_df.iterrows():\n        df.loc[idx, 'Age'] = median_age_dict[row['title_tmp']]\n    # Binning\n    df['age_tmp'] = pd.qcut(df['Age'], n_bins)\n    label = preprocessing.LabelEncoder()\n    df['AgeBinned'] = label.fit_transform(df['age_tmp'])\n    # Drop tmp columns\n    df.drop(columns=['tmp', 'title_tmp', 'age_tmp'], inplace=True)\n    return df\n\n\ndef fine_tuning(df):\n    df['last_name'] = df['Name'].apply(lambda x:str.split(x,\",\")[0]) #multiple Python features in play here. How cool is that!!\n    DEFAULT_SURVIVAL_VALUE = 0.5\n    df['FamilySurvival'] = DEFAULT_SURVIVAL_VALUE\n    for grp, grp_df in df[['Survived','Name', 'last_name', 'Fare', 'Ticket', 'PassengerId','Parch','SibSp', 'Age', 'Cabin']].groupby(['last_name','Fare']):\n        if (len(grp_df) != 1): #more than one in a group   \n            for ind, row in grp_df.iterrows():\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if smax == 1.0:\n                    df.loc[df['PassengerId'] == passID,'FamilySurvival'] = 1\n                elif (smin == 0.0):\n                    df.loc[df['PassengerId']== passID, 'FamilySurvival'] = 0\n    for _, grp_df in df.groupby('Ticket'):\n        if (len(grp_df) != 1):\n            for ind, row in grp_df.iterrows():\n                if (row['FamilySurvival'] == 0) | (row['FamilySurvival'] == 0.5):\n                    smax = grp_df.drop(ind)['Survived'].max()\n                    smin = grp_df.drop(ind)['Survived'].min()\n                    passID = row['PassengerId']\n                    if (smax == 1.0):\n                        df.loc[df['PassengerId'] == passID,'FamilySurvival'] = 1\n                    elif(smin == 0.0):\n                        df.loc[df['PassengerId'] == passID,'FamilySurvival'] = 0\n    df.drop(columns=['last_name'], inplace=True)\n    return df\n\n\ndef preprocess_df(df, fare_bins, age_bins):\n    df = add_cabin_letter(df)\n    df = add_title(df)\n    df = add_family_size(df)\n    df = add_group_size(df)\n    df = process_fare(df, fare_bins)\n    df = fill_nan_in_embarked(df)\n    df = process_age(df, age_bins)\n    df = fine_tuning(df)\n    df['isMale'] = (df['Sex'] == 'male').astype(int)\n    embarked_label = preprocessing.LabelEncoder()\n    df['Embarked'] = embarked_label.fit_transform(df['Embarked'])\n    df.drop(columns=['Name', 'PassengerId', 'Sex', 'Cabin', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Age'], inplace=True)\n    return df\n\ntrain_df = pd.read_csv('../input/titanic/train.csv')\ntest_df = pd.read_csv('../input/titanic/test.csv')\ntrain_n_rows = train_df.shape[0]\n\nfull_df = pd.concat([train_df, test_df])\nfull_df = preprocess_df(full_df, 6, 6)\n\ntrain_df = full_df.iloc[:train_n_rows]\ny_train = train_df['Survived'].values\nX_train = train_df.drop(columns=['Survived']).values\nX_test = full_df.iloc[train_n_rows:].drop(columns=['Survived']).values\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_df.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:04:30.608288Z","iopub.execute_input":"2022-04-23T15:04:30.608846Z","iopub.status.idle":"2022-04-23T15:04:33.214539Z","shell.execute_reply.started":"2022-04-23T15:04:30.608812Z","shell.execute_reply":"2022-04-23T15:04:33.213759Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection\n\nFor this kind of binary classification task, KNN, random forest, XGBoost and logistic regression algorithms are usually used.\n\nI've used a grid search/random search to tune hyper-parameters for each model, and select the one with the highest ROC_AUC score.","metadata":{}},{"cell_type":"markdown","source":"### 1. KNN","metadata":{}},{"cell_type":"code","source":"hyperparams = {\n    'algorithm': ['auto'], \n    'weights': ['uniform','distance'], \n    'leaf_size': [i for i in range(1, 55, 5)],\n    'n_neighbors': [i for i in range(4, 50, 2)]\n}\nknn_grid_search = GridSearchCV(\n    estimator=KNeighborsClassifier(),\n    param_grid=hyperparams,\n    verbose=True,\n    cv=10,\n    n_jobs=-1,\n    scoring=\"roc_auc\"\n)\nknn_grid_search.fit(X_train,y_train)\n\nprint(\"KNN\")\nprint(\"Best parameters: \", knn_grid_search.best_estimator_)\nprint(\"Best ROC_AUC: \", knn_grid_search.best_score_)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:08:26.974917Z","iopub.execute_input":"2022-04-23T15:08:26.975220Z","iopub.status.idle":"2022-04-23T15:08:55.201507Z","shell.execute_reply.started":"2022-04-23T15:08:26.975170Z","shell.execute_reply":"2022-04-23T15:08:55.200599Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### 2. Radom Forest","metadata":{}},{"cell_type":"code","source":"hyperparams = {\n    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n    'max_features': ['auto', 'sqrt', 'log2'] + [i for i in range(1, 26)],\n    'max_depth': [i for i in range(2, 500, 2)] + [None],\n    'min_samples_split': [i for i in range(2, 200, 2)],\n    'max_leaf_nodes': [i for i in range(2, 2000, 4)],\n    'max_samples': [i * 0.1 for i in range(1, 10)],\n    'bootstrap': [True]\n}\nrf = RandomForestClassifier()\nrf_randsearch = RandomizedSearchCV(\n    estimator=rf, \n    param_distributions=hyperparams, \n    n_iter=100, \n    cv=10,\n    random_state=42, \n    n_jobs=-1,\n    scoring='roc_auc'\n)\nrf_randsearch.fit(X_train, y_train)\nprint(\"Random Forest\")\nprint(\"Best parameters: \", rf_randsearch.best_estimator_)\nprint(\"Best ROC_AUC: \", rf_randsearch.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. XGBoost","metadata":{}},{"cell_type":"code","source":"hyperparams = {\n    'max_depth': [3, 6, 10, 20, 40, 80],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8],\n    'n_estimators': [100, 500, 1000, 2000],\n    'colsample_bytree': [0.1, 0.3, 0.7, 1]\n}\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", verbosity=0)\nxgb_randsearch = RandomizedSearchCV(\n    estimator=xgb_model, \n    param_distributions=hyperparams,\n    scoring='roc_auc',\n    cv=10,\n    n_iter=100,\n    n_jobs=-1\n)\nxgb_randsearch.fit(X_train, y_train)\nprint(\"XGBoost\")\nprint(\"Best parameters: \", xgb_randsearch.best_estimator_)\nprint(\"Best ROC_AUC: \", xgb_randsearch.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"#### 4.1 L2 Regularization (Ridge Regression)","metadata":{}},{"cell_type":"code","source":"l2_grid = {\n    'penalty': ['l2'],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    'max_iter': [i for i in range(100, 5000, 100)],\n    'warm_start': [True, False],\n    'class_weight': [None, 'auto'],\n    'C': [i * 0.1 for i in range(1, 50)]\n}\nl2 = LogisticRegression()\nl2_randsearch = RandomizedSearchCV(\n    estimator=l2, \n    param_distributions=l2_grid, \n    n_iter=100, \n    cv=3,\n    random_state=42,\n    n_jobs=-1,\n    scoring='roc_auc'\n)\nl2_randsearch.fit(X_train, y_train)\nprint(\"Logistic Regression with L2 Regularization\")\nprint(\"Best parameters: \", l2_randsearch.best_estimator_)\nprint(\"Best ROC_AUC: \", l2_randsearch.best_score_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2 L1 Regularization (Lasso Regression)","metadata":{}},{"cell_type":"code","source":"l1_grid = {\n    'penalty': ['l1'],\n    'solver': ['liblinear', 'saga'],\n    'max_iter': [i for i in range(100, 5000, 100)],\n    'warm_start': [True, False],\n    'class_weight': [None, 'auto'],\n    'C': [i * 0.1 for i in range(1, 50)]\n}\nl1 = LogisticRegression()\nl1_randsearch = RandomizedSearchCV(\n    estimator=l1, \n    param_distributions=l1_grid, \n    n_iter=100, \n    cv=3,\n    random_state=42, \n    n_jobs=-1,\n    scoring='roc_auc'\n)\nl1_randsearch.fit(X_train, y_train)\nprint(\"Logistic Regression with L1 Regularization\")\nprint(\"Best parameters: \", l1_randsearch.best_estimator_)\nprint(\"Best ROC_AUC: \", l1_randsearch.best_score_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.3 L1 & L2 Regularization","metadata":{}},{"cell_type":"code","source":"l12_grid = {\n    'penalty': ['elasticnet'],\n    'solver': ['saga'],\n    'max_iter': [i for i in range(100, 5000, 100)],\n    'warm_start': [True, False],\n    'class_weight': [None, 'auto'],\n    'C': [i * 0.1 for i in range(1, 50)],\n    'l1_ratio': [i * 0.1 for i in range(1, 10)]\n}\nl12 = LogisticRegression()\nl12_randsearch = RandomizedSearchCV(\n    estimator=l12, \n    param_distributions=l12_grid, \n    n_iter=100, \n    cv=10,\n    random_state=42, \n    n_jobs=-1,\n    scoring='roc_auc'\n)\nl12_randsearch.fit(X_train, y_train)\nprint(\"Logistic Regression with L1 & L2 Regularization\")\nprint(\"Best parameters: \", l12_randsearch.best_estimator_)\nprint(\"Best ROC_AUC: \", l12_randsearch.best_score_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Submission File","metadata":{}},{"cell_type":"code","source":"def generate_submission_file(model, X_train, y_train, X_test, filename):\n    print(f\"Generating {filename} ...\")\n    model.best_estimator_.fit(X_train, y_train)\n    y_pred = model.best_estimator_.predict(X_test)\n    output_df = pd.DataFrame(pd.read_csv('titanic/test.csv')['PassengerId'])\n    output_df['Survived'] = y_pred.astype(int)\n    output_df.to_csv(filename, index=False)\n    print(\"Done\")\n    return True\n\nmodel_fn_lst = [\n    [knn_grid_search, 'submission/KNN_submission.csv'],\n    [xgb_randsearch, 'submission/XGBoost_submission.csv'],\n    [rf_randsearch, 'submission/RandomForest_submission.csv'],\n    [l2_randsearch, 'submission/RidgeRegression_submission.csv'],\n    [l1_randsearch, 'submission/LassoRegression_submission.csv'],\n    [l12_randsearch, 'submission/L12Regression_submission.csv']\n]\nfor model, out_fn in model_fn_lst:\n    generate_submission_file(model, X_train, y_train, X_test, out_fn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}